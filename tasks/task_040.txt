# Task ID: 40
# Title: Implement LLM Service Retry Mechanism with Exponential Backoff
# Status: pending
# Dependencies: None
# Priority: high
# Description: Add a robust retry mechanism with exponential backoff to BaseLLMService implementations for handling transient LLM API errors
# Details:
As specified in PRD v3.16 section 3.3 under \"[MVP Polish - Dev/UX]: (Retry Backoff Strategy)\", implement a retry mechanism with exponential backoff for BaseLLMService concrete implementations:\n\n1. Create a generic retry utility in forest_app/utils/retry.py:\n```python\nfrom typing import TypeVar, Callable, Any, List, Dict, Optional\nimport time\nimport random\nimport logging\n\nT = TypeVar('T')\n\ndef retry_with_exponential_backoff(\n    func: Callable[..., T],\n    max_retries: int = 3,\n    initial_delay_ms: int = 500,\n    max_delay_ms: int = 8000,\n    backoff_factor: float = 2.0,\n    jitter_factor: float = 0.1,\n    retryable_exceptions: List[type] = None,\n    retry_condition: Callable[[Exception], bool] = None,\n    logger: Optional[logging.Logger] = None\n) -> T:\n    \"\"\"Execute a function with exponential backoff retry strategy.\n    \n    Args:\n        func: The function to execute\n        max_retries: Maximum number of retries before giving up\n        initial_delay_ms: Initial delay in milliseconds\n        max_delay_ms: Maximum delay in milliseconds\n        backoff_factor: Multiplicative factor for each retry delay\n        jitter_factor: Random jitter factor to add to delay (as a percentage)\n        retryable_exceptions: List of exception types that should trigger a retry\n        retry_condition: Optional callable to determine if an exception is retryable\n        logger: Optional logger for logging retries\n        \n    Returns:\n        The result of the function\n        \n    Raises:\n        The last exception encountered after all retries are exhausted\n    \"\"\"\n    retryable_exceptions = retryable_exceptions or []\n    logger = logger or logging.getLogger(__name__)\n    \n    last_exception = None\n    \n    for attempt in range(max_retries + 1):  # +1 for the initial attempt\n        try:\n            return func()\n            \n        except Exception as e:\n            last_exception = e\n            \n            # Check if we should retry based on exception type or custom condition\n            should_retry = False\n            if any(isinstance(e, exc_type) for exc_type in retryable_exceptions):\n                should_retry = True\n            if retry_condition and retry_condition(e):\n                should_retry = True\n                \n            # Don't retry if it's not a retryable exception or on last attempt\n            if not should_retry or attempt == max_retries:\n                raise\n                \n            # Calculate delay with exponential backoff and jitter\n            delay_ms = min(initial_delay_ms * (backoff_factor ** attempt), max_delay_ms)\n            jitter_ms = random.uniform(-jitter_factor * delay_ms, jitter_factor * delay_ms)\n            total_delay_ms = delay_ms + jitter_ms\n            \n            logger.warning(\n                f\"Retry attempt {attempt+1}/{max_retries} for operation after {total_delay_ms:.0f}ms. \"\n                f\"Error: {str(e)}\"\n            )\n            \n            time.sleep(total_delay_ms / 1000)  # Convert to seconds\n            \n    # This should not be reached due to the raise in the loop, but just in case\n    if last_exception:\n        raise last_exception\n    \n    raise RuntimeError(\"Unexpected error in retry_with_exponential_backoff\")\n```\n\n2. Integrate the retry mechanism into GoogleGeminiService:\n```python\nclass GoogleGeminiService(BaseLLMService):\n    def __init__(self, api_key: str, project_id: str = None, location: str = None, retry_config: Dict[str, Any] = None):\n        self.api_key = api_key\n        self.project_id = project_id\n        self.location = location\n        self.retry_config = retry_config or {\n            \"max_retries\": 3,\n            \"initial_delay_ms\": 500,\n            \"max_delay_ms\": 8000,\n            \"backoff_factor\": 2.0,\n            \"jitter_factor\": 0.1\n        }\n        self.logger = logging.getLogger(__name__)\n        self._setup_gemini_client()\n    \n    def generate_text(self, prompt: str, max_output_tokens: int = 1024, **kwargs) -> str:\n        \"\"\"Generate text using Google Gemini with retry logic for transient errors.\"\"\"\n        def _generate():\n            # Actual implementation of the Gemini API call\n            # ...\n            return response_text\n        \n        # List of retryable exceptions specific to Gemini API\n        retryable_exceptions = [\n            # Add specific exception types for transient Gemini errors\n            # e.g., google.api_core.exceptions.ServiceUnavailable,\n            #      google.api_core.exceptions.ResourceExhausted,\n            #      Timeout errors, etc.\n        ]\n        \n        # Custom condition for determining if an error is retryable\n        def is_retryable(e):\n            # Logic to determine if the error is transient/retryable\n            # e.g., check for rate limiting, temporary unavailability, etc.\n            return False  # Default implementation\n        \n        start_time = time.time()\n        \n        try:\n            result = retry_with_exponential_backoff(\n                func=_generate,\n                max_retries=self.retry_config[\"max_retries\"],\n                initial_delay_ms=self.retry_config[\"initial_delay_ms\"],\n                max_delay_ms=self.retry_config[\"max_delay_ms\"],\n                backoff_factor=self.retry_config[\"backoff_factor\"],\n                jitter_factor=self.retry_config[\"jitter_factor\"],\n                retryable_exceptions=retryable_exceptions,\n                retry_condition=is_retryable,\n                logger=self.logger\n            )\n            \n            elapsed_ms = (time.time() - start_time) * 1000\n            self.logger.info(f\"Generated text in {elapsed_ms:.0f}ms with Gemini API\")\n            return result\n            \n        except Exception as e:\n            self.logger.error(f\"Failed to generate text after retries: {str(e)}\")\n            raise\n```\n\n3. Apply similar logic to other LLM service implementations:\n   - Modify other services to use the retry mechanism\n   - Configure appropriate retry settings for each service type\n   - Identify specific retryable exceptions for each LLM provider\n\n4. Add monitoring and logging for retry attempts:\n   - Log detailed information about each retry\n   - Track successful retry recoveries for telemetry\n   - Record retry patterns to tune the configuration\n\n5. Make retry settings configurable:\n   - Add retry configuration to settings.py\n   - Allow environment variable overrides for settings\n   - Document retry configuration options\n\nThis implementation will help maintain reliable service through transient API errors, improving the overall robustness of the LLM interactions that are critical to the system.

# Test Strategy:

