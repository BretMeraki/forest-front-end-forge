"""Initial schema check after adding IP

Revision ID: f5b76ed1b9bd
Revises:
Create Date: 2025-04-29 13:04:39.696706

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa
# Import func if needed for server defaults, though NOW() is standard SQL
# from sqlalchemy.sql import func
from sqlalchemy.dialects import postgresql

# revision identifiers, used by Alembic.
revision: str = 'f5b76ed1b9bd'
down_revision: Union[str, None] = None
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    """Upgrade schema."""
    # ### commands auto generated by Alembic - please adjust! ###
    op.create_table('reflection_logs',
    sa.Column('id', postgresql.UUID(as_uuid=True), nullable=False, server_default=sa.text('gen_random_uuid()')),
    sa.Column('user_id', postgresql.UUID(as_uuid=True), nullable=False),
    sa.Column('timestamp', sa.DateTime(timezone=True), server_default=sa.text('now()'), nullable=False),
    sa.Column('reflection_text', sa.Text(), nullable=False),
    sa.Column('snapshot_ref', postgresql.JSONB(astext_type=sa.Text()), nullable=True),
    sa.Column('analysis_metadata', postgresql.JSONB(astext_type=sa.Text()), nullable=True),
    sa.ForeignKeyConstraint(['user_id'], ['users.id'], ),
    sa.PrimaryKeyConstraint('id')
    )
    op.create_index(op.f('ix_reflection_logs_id'), 'reflection_logs', ['id'], unique=False)
    op.create_index(op.f('ix_reflection_logs_user_id'), 'reflection_logs', ['user_id'], unique=False)
    op.create_table('task_footprints',
    sa.Column('id', postgresql.UUID(as_uuid=True), nullable=False, server_default=sa.text('gen_random_uuid()')),
    sa.Column('user_id', postgresql.UUID(as_uuid=True), nullable=False),
    sa.Column('task_id', sa.String(), nullable=False),
    sa.Column('event_type', sa.String(), nullable=False),
    sa.Column('timestamp', sa.DateTime(timezone=True), server_default=sa.text('now()'), nullable=False),
    sa.Column('snapshot_ref', postgresql.JSONB(astext_type=sa.Text()), nullable=True),
    sa.Column('event_metadata', postgresql.JSONB(astext_type=sa.Text()), nullable=True),
    sa.ForeignKeyConstraint(['user_id'], ['users.id'], ),
    sa.PrimaryKeyConstraint('id')
    )
    op.create_index(op.f('ix_task_footprints_id'), 'task_footprints', ['id'], unique=False)
    op.create_index(op.f('ix_task_footprints_task_id'), 'task_footprints', ['task_id'], unique=False)
    op.create_index(op.f('ix_task_footprints_user_id'), 'task_footprints', ['user_id'], unique=False)
    op.drop_index('ix_reflection_event_logs_id', table_name='reflection_event_logs')
    op.drop_index('ix_reflection_event_logs_linked_hta_node_id', table_name='reflection_event_logs')
    op.drop_index('ix_reflection_event_logs_reflection_id', table_name='reflection_event_logs')
    op.drop_index('ix_reflection_event_logs_timestamp', table_name='reflection_event_logs')
    op.drop_table('reflection_event_logs')
    op.drop_index('ix_task_event_logs_id', table_name='task_event_logs')
    op.drop_index('ix_task_event_logs_linked_hta_node_id', table_name='task_event_logs')
    op.drop_index('ix_task_event_logs_task_id', table_name='task_event_logs')
    op.drop_index('ix_task_event_logs_timestamp', table_name='task_event_logs')
    op.drop_table('task_event_logs')
    op.alter_column('memory_snapshots', 'snapshot_data',
               existing_type=postgresql.JSON(astext_type=sa.Text()),
               type_=postgresql.JSONB(astext_type=sa.Text()),
               nullable=True)
    # --- Note: Handling potential NULLs before altering memory_snapshots.created_at ---
    # If created_at could also have NULLs, you'd add a similar op.execute() here:
    # op.execute("UPDATE memory_snapshots SET created_at = NOW() WHERE created_at IS NULL")
    op.alter_column('memory_snapshots', 'created_at',
               existing_type=postgresql.TIMESTAMP(),
               type_=sa.DateTime(timezone=True),
               nullable=False,
               server_default=sa.text('now()')) # Added server_default for consistency
    op.drop_index('ix_memory_snapshots_codename', table_name='memory_snapshots')
    # Assuming the foreign key constraint name needs to be generated or is known
    # If None doesn't work, you might need to specify a name like 'fk_memory_snapshots_user_id_users'
    op.create_foreign_key(None, 'memory_snapshots', 'users', ['user_id'], ['id'])
    op.drop_column('memory_snapshots', 'updated_at')

    # --- Note: Handling potential NULLs before altering users.is_active ---
    # If is_active could have NULLs and needs a default (e.g., True)
    # op.execute("UPDATE users SET is_active = TRUE WHERE is_active IS NULL")
    op.alter_column('users', 'is_active',
               existing_type=sa.BOOLEAN(),
               nullable=False,
               server_default=sa.true()) # Added server_default for consistency

    # --- Note: Handling potential NULLs before altering users.created_at ---
    # If created_at could also have NULLs, you'd add a similar op.execute() here:
    # op.execute("UPDATE users SET created_at = NOW() WHERE created_at IS NULL")
    op.alter_column('users', 'created_at',
               existing_type=postgresql.TIMESTAMP(),
               type_=sa.DateTime(timezone=True),
               nullable=False,
               server_default=sa.text('now()')) # Added server_default for consistency

    # --- FIX FOR users.updated_at NULLS ---
    # Update existing rows where updated_at is NULL before applying NOT NULL constraint
    print("Updating NULL values in users.updated_at...") # Optional: for logging
    op.execute(
        "UPDATE users SET updated_at = NOW() WHERE updated_at IS NULL"
    )
    print("Finished updating NULL values.") # Optional: for logging
    # --- END FIX ---

    # Now apply the NOT NULL constraint (and potentially type change)
    op.alter_column('users', 'updated_at',
               existing_type=postgresql.TIMESTAMP(timezone=True), # Assuming this type is correct from models.py
               nullable=False,
               server_default=sa.text('now()'), # Added server_default for consistency
               existing_server_default=sa.text('now()')) # Added existing for onupdate

    op.drop_column('users', 'full_name')
    # ### end Alembic commands ###


def downgrade() -> None:
    """Downgrade schema."""
    # ### commands auto generated by Alembic - please adjust! ###
    # Note: Reversing the update for NULLs isn't straightforward,
    # as we don't know which ones were originally NULL.
    # The downgrade path might need manual adjustment if this migration
    # needs to be reversible in practice.
    op.add_column('users', sa.Column('full_name', sa.VARCHAR(), autoincrement=False, nullable=True))
    op.alter_column('users', 'updated_at',
               existing_type=postgresql.TIMESTAMP(timezone=True),
               nullable=True, # Revert nullable back to True
               server_default=None, # Remove server default if it was added
               existing_server_default=sa.text('now()')) # Keep existing if needed? Check model
    op.alter_column('users', 'created_at',
               existing_type=sa.DateTime(timezone=True),
               type_=postgresql.TIMESTAMP(),
               nullable=True, # Revert nullable back to True
               server_default=None) # Remove server default
    op.alter_column('users', 'is_active',
               existing_type=sa.BOOLEAN(),
               nullable=True, # Revert nullable back to True
               server_default=None) # Remove server default
    op.add_column('memory_snapshots', sa.Column('updated_at', postgresql.TIMESTAMP(), autoincrement=False, nullable=True))
    # Need the actual constraint name to drop it reliably
    # Replace 'fk_memory_snapshots_user_id_users' with the correct name if needed
    op.drop_constraint('fk_memory_snapshots_user_id_users', 'memory_snapshots', type_='foreignkey')
    op.create_index('ix_memory_snapshots_codename', 'memory_snapshots', ['codename'], unique=False)
    op.alter_column('memory_snapshots', 'created_at',
               existing_type=sa.DateTime(timezone=True),
               type_=postgresql.TIMESTAMP(),
               nullable=True, # Revert nullable back to True
               server_default=None) # Remove server default
    op.alter_column('memory_snapshots', 'snapshot_data',
               existing_type=postgresql.JSONB(astext_type=sa.Text()),
               type_=postgresql.JSON(astext_type=sa.Text()),
               nullable=True) # Assuming it was nullable before? Check old model/DB state
    op.create_table('task_event_logs',
        # ... (rest of the downgrade definition, ensure it matches the old state) ...
        # This part is complex and depends heavily on the exact previous state
        # It's often safer to restore from backup than to rely on complex downgrades
        # Placeholder for brevity:
        sa.Column('id', sa.INTEGER(), autoincrement=True, nullable=False),
        # Add other columns from the original task_event_logs definition
        sa.PrimaryKeyConstraint('id', name='task_event_logs_pkey')
    )
    # Recreate indexes for task_event_logs
    op.create_table('reflection_event_logs',
        # Placeholder for brevity:
        sa.Column('id', sa.INTEGER(), autoincrement=True, nullable=False),
        # Add other columns from the original reflection_event_logs definition
        sa.PrimaryKeyConstraint('id', name='reflection_event_logs_pkey')
    )
    # Recreate indexes for reflection_event_logs
    op.drop_index(op.f('ix_task_footprints_user_id'), table_name='task_footprints')
    op.drop_index(op.f('ix_task_footprints_task_id'), table_name='task_footprints')
    op.drop_index(op.f('ix_task_footprints_id'), table_name='task_footprints')
    op.drop_table('task_footprints')
    op.drop_index(op.f('ix_reflection_logs_user_id'), table_name='reflection_logs')
    op.drop_index(op.f('ix_reflection_logs_id'), table_name='reflection_logs')
    op.drop_table('reflection_logs')
    # ### end Alembic commands ###

